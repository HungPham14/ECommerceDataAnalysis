{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM Customer Segmentation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFM (Recency, Frequency, and Monetary) analysis is a popular method for customer segmentation that helps businesses understand customer behavior and target them more effectively. It's used primarily to evaluate customer value and predict future purchasing behavior by analyzing three key aspects:\n",
    "\n",
    "- Recency (R): How recently a customer made a purchase. Customers who have purchased recently are generally more likely to buy again compared to those who haven't purchased in a long time.\n",
    "\n",
    "- Frequency (F): How often a customer makes a purchase. A high-frequency customer is often a loyal customer, which can lead to higher lifetime value.\n",
    "\n",
    "- Monetary (M): How much money a customer spends. Customers who spend more are considered high-value and should be prioritized in retention and upselling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCI Online Retail Dataset\n",
    "\n",
    "This is a transnational data set which contains all the transactions occurring from December 1st 2010 until December 9th 2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers. Each row represents the transaction that occurs. It includes the product name, quantity, price, and other columns that represents ID.\n",
    "\n",
    "Source: http://archive.ics.uci.edu/ml/datasets/Online+Retail (Dr Daqing Chen, Director: Public Analytics group. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Show all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "# Data Description for each column\n",
    "column_description = pd.DataFrame({\n",
    "    'Column Name':['InvoiceNo','StockCode','Description','Quantity','InvoiceDate','UnitPrice','CustomerID','Country']\n",
    "    ,'Description':[\n",
    "        'Invoice number.If this code starts with letter \"c\", it indicates a cancellation.'\n",
    "        ,'Product (item) code'\n",
    "        ,'Product (item) name.'\n",
    "        ,'The quantities of each product (item) per transaction.'\n",
    "        ,'Invice Date and time'\n",
    "        ,'Unit price'\n",
    "        ,'Customer number'\n",
    "        ,'Country name'\n",
    "    ]\n",
    "    ,'Data Type':[\n",
    "        'Nominal, a 6-digit integral number uniquely assigned to each transaction'\n",
    "        ,'Nominal, a 5-digit integral number uniquely assigned to each distinct product'\n",
    "        ,'Nominal'\n",
    "        ,'Numeric'\n",
    "        ,'Numeric, the day and time when each transaction was generated'\n",
    "        ,'Numeric, Product price per unit in sterling'\n",
    "        ,'Nominal, a 5-digit integral number uniquely assigned to each customer'\n",
    "        ,'Nominal, the name of the country where each customer reside'\n",
    "    ]\n",
    "})\n",
    "column_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process involves checking for errors, inconsistencies, and missing values in the data to ensures the accuracy, quality, and integrity of the data by checking duplicates, missing data, structural errors, outliers of the dataset.\n",
    "\n",
    "Data Validation is necessary to produce clean and reliable data that is ready for analysis or further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import seaborn as sns # data visualization\n",
    "\n",
    "# display full outputs in Jupyter Notebook, not only the last command's output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "ecommerce_data = pd.read_csv(r'data\\\\E-Commerce Data.csv', encoding='ISO-8859-1')\n",
    "ecommerce_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of duplicated rows: {ecommerce_data.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate sample row\n",
    "ecommerce_data[ecommerce_data.duplicated()].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_data[(ecommerce_data['InvoiceNo'] == '536409') & (ecommerce_data['StockCode'] == '21866')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those duplicated records can skew our analysis. Therefore, let's drop these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_data.drop_duplicates(inplace=True)\n",
    "print(f'Number of duplicated rows: {ecommerce_data.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = ecommerce_data.isnull().sum().to_frame().rename(columns={0: 'total'})\n",
    "missing_data['percent'] = (missing_data['total'] / ecommerce_data.shape[0]) * 100   \n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomerID has the highest null value taking up to 25% of total records, we can try:\n",
    "1. Drop all null value rows\n",
    "2. Try to find ways to fill those Nan values\n",
    "\n",
    "With option 2 in mind, I have a theory to fill in values. For example, InvoiceNo A has CustomerID A and Customer null, I can replace these NaN CustomerID with existing CustomerID A since it's just missing input error. \n",
    "\n",
    "If option 2 failed, I will do option 1 which is dropping all null value rows for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = ecommerce_data[ecommerce_data['CustomerID'].isnull()]\n",
    "null_df.reset_index(drop=True, inplace=True)\n",
    "null_df.shape\n",
    "null_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_df=ecommerce_data[ecommerce_data['CustomerID'].isnull() == False]\n",
    "not_null_df.reset_index(drop=True, inplace=True)\n",
    "not_null_df.shape\n",
    "not_null_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Find intersection between 2 dataframes with same InvoiceNo values\n",
    "null_df[null_df['InvoiceNo'].isin(not_null_df['InvoiceNo'].unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no repeated Invoice Number between the not null and the null customerID data. Therefore, we can not perform any imputation here, this leaves us no choice but to remove these null records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing CustomerID\n",
    "ecommerce_data = ecommerce_data[~ecommerce_data['CustomerID'].isnull()]\n",
    "ecommerce_data.reset_index(drop=True, inplace=True)\n",
    "ecommerce_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values again\n",
    "missing_data = ecommerce_data.isnull().sum().to_frame().rename(columns={0: 'total'})\n",
    "missing_data['percent'] = (missing_data['total'] / ecommerce_data.shape[0]) * 100   \n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Datetime Datatype Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Description of the Dataset, InvoiceDate is the only column containing datetime datatype and is extremely crucial for generating Recency, Frequency feature of RFM Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime # datetime manipulation\n",
    "\n",
    "# validate date format\n",
    "def validate_datetime(d):\n",
    "    try:\n",
    "        datetime.datetime.strptime('4/21/2011 19:05', '%m/%d/%Y %H:%M')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "ecommerce_data[~ecommerce_data['InvoiceDate'].apply(lambda x: validate_datetime(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our datetime data is validated with '%m/%d/%Y %H:%M' format, let's convert them from object to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime conversion\n",
    "ecommerce_data['InvoiceDate'] = pd.to_datetime(ecommerce_data['InvoiceDate'],format='%m/%d/%Y %H:%M')\n",
    "ecommerce_data['InvoiceDate'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Object Datatype Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the description of dataset, there are 5 Columns containing Nominal datatype:\n",
    "- InvoiceNo\n",
    "- StockCode\n",
    "- Description\n",
    "- CustomerID\n",
    "- Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of categorical columns\n",
    "print(f'There are currently only {ecommerce_data.select_dtypes(include=\"object\").columns.size} Nominal columns\\n')\n",
    "\n",
    "# Check the unique values of each categorical column\n",
    "for cate_col in ecommerce_data.select_dtypes(include='object').columns:\n",
    "    print(f'{cate_col}: {ecommerce_data[cate_col].nunique()} unique values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert CustomerID into Nominal type too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_data['CustomerID'] = ecommerce_data['CustomerID'].astype(str)\n",
    "# Check the number of categorical columns\n",
    "print(f'There are currently {ecommerce_data.select_dtypes(include=\"object\").columns.size} Nominal columns\\n')\n",
    "\n",
    "# Check the unique values of each categorical column\n",
    "for cate_col in ecommerce_data.select_dtypes(include='object').columns:\n",
    "    print(f'{cate_col}: {ecommerce_data[cate_col].nunique()} unique values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Checking Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_data.select_dtypes([int,float]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec # subplots\n",
    "\n",
    "# Create 2x2 sub plots\n",
    "gs = gridspec.GridSpec(2, 1)\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "# add axes_1\n",
    "ax1 = plt.subplot(gs[0, 0]) \n",
    "ax1 = sns.boxplot(\n",
    "    data=ecommerce_data.UnitPrice\n",
    "    ,orient='h'\n",
    ")\n",
    "\n",
    "# add axes_2\n",
    "ax2 = plt.subplot(gs[1, 0]) \n",
    "ax2 = sns.boxplot(\n",
    "    data=ecommerce_data.Quantity\n",
    "    ,orient='h'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As box plot shown above, there a number of records with negative values of quantity. These negative values could be indicates a cancellation, discount, which from RFM Analysis aspect, we may need some transaction such as discount to reflect the actual customer behaviour. So these negative values could be true outliers, not incorrect, structural errors caused by data entry, processing stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process involves removing or excluding data that does not meet certain conditions or criteria. Includes filtering based on values, ranges, or conditions (e.g., removing outliers, selecting specific time periods) with the purpose to select only a subset of data based on specific criteria to focus on relevant information in order to produce a refined dataset that contains only the relevant information needed for analysis, in this case, for the RFM analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lowercase Description\n",
    "ecommerce_data[ecommerce_data['Description'].str.contains('[a-z]')]['Description'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description such as Manual, Bank Charges, etc is not product, therefore, let's gather these values into a non product list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_products = ['Next Day Carriage', \n",
    "                'Discount', \n",
    "                'CRUK Commission', \n",
    "                'Bank Charges', \n",
    "                'Manual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check short Description\n",
    "def short_description(desc):\n",
    "    return len(desc.split()) < 3\n",
    "ecommerce_data[\n",
    "    (ecommerce_data['Description'].apply(lambda x: short_description(x))) &\n",
    "    (~ecommerce_data['Description'].isin(not_products))\n",
    "]['Description'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'POSTAGE', 'DOTCOM POSTAGE' and 'CARRIAGE' are not products. So I will have them appended to the not_products list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_products.append('POSTAGE')\n",
    "not_products.append('CARRIAGE')\n",
    "not_products.append('DOTCOM POSTAGE')\n",
    "not_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_data[ecommerce_data['Description'] == 'Manual'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. RFM Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
